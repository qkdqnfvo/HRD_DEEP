{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from Activation import Activation\n",
    "\n",
    "class Network:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.W = {\n",
    "            'W1':np.random.randn(4,100),\n",
    "            'W2':np.random.randn(100,50),\n",
    "            'W3':np.random.randn(50,3)\n",
    "        }\n",
    "        self.b = {\n",
    "            'b1':np.random.randn(100),\n",
    "            'b2':np.random.randn(50),\n",
    "            'b3':np.random.randn(3)\n",
    "        }\n",
    "        self.Activation ={\n",
    "            'sigmoid':Activation.sigmoid,\n",
    "            'relu':Activation.relu,\n",
    "            'softmax':Activation.softmax\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def predict(self,x):\n",
    "        result = np.dot(x,self.W['W1']) + self.b['b1']\n",
    "        for i in range(len(self.W)-2):\n",
    "            result = np.dot(result,self.W['W'+str(i+2)]) + self.b['b'+str(i+2)]\n",
    "            result = Activation.relu(result)    \n",
    "        result = np.dot(result,self.W['W'+str(len(self.W))]) + self.b['b'+str(len(self.W))]    \n",
    "        return Activation.softmax(result)\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        self.y = self.predict(x)\n",
    "        loss = cross_entropy(self.y,t)\n",
    "        return loss\n",
    "    \n",
    "    def gradient(self,x,t,learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        f = lambda w : self.loss(x,t)\n",
    "        for i in range(len(self.W)):\n",
    "            self.W['W'+str(i+1)] -= self.learning_rate*numerical_gradient(f,self.W['W'+str(i+1)])\n",
    "            self.b['b'+str(i+1)] -= self.learning_rate*numerical_gradient(f,self.b['b'+str(i+1)])\n",
    "            \n",
    "        \n",
    "#     def fit(self,x,t,learning_rate,epochs):\n",
    "#         for epoch in range(epochs):\n",
    "#             self.gradient(self,x,t)\n",
    "        \n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        result = self.predict(x)\n",
    "        acc = sum(np.argmax(result,axis=1) == np.argmax(t,axis=1))/len(t)\n",
    "        return acc\n",
    "\n",
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x,flags=['multi_index'],op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1-fxh2)/(2*h)\n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "def par_der(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x,flags=['multi_index'],op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "        x[idx] = float(tmp_val) - h\n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1 - fxh2)/(2*h)\n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "def cross_entropy(y,t):\n",
    "    epsilon = 1e-7\n",
    "    # y = Activation.softmax(y)\n",
    "    return -np.sum(t*np.log(y+epsilon))/y.shape[0]\n",
    "\n",
    "##data\n",
    "def make_onehot(x):\n",
    "    col = np.unique(x).size\n",
    "    row = x.size\n",
    "    data = np.zeros((row,col))\n",
    "    for i in np.arange(row):\n",
    "        data[i,x[i]] = 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 1.2101 - accuracy: 0.3333\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1145 - accuracy: 0.3333\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0559 - accuracy: 0.3333\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0172 - accuracy: 0.5533\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9866 - accuracy: 0.5800\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9612 - accuracy: 0.4867\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.9412 - accuracy: 0.4133\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.9236 - accuracy: 0.4000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.9072 - accuracy: 0.3733\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8921 - accuracy: 0.3667\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8779 - accuracy: 0.3600\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8643 - accuracy: 0.3533\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8513 - accuracy: 0.5133\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8390 - accuracy: 0.6067\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8274 - accuracy: 0.6733\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.8167 - accuracy: 0.6867\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.8068 - accuracy: 0.7000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7975 - accuracy: 0.7000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7886 - accuracy: 0.7067\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7800 - accuracy: 0.7133\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7717 - accuracy: 0.7467\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7638 - accuracy: 0.7467\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7563 - accuracy: 0.7467\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7491 - accuracy: 0.7467\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7422 - accuracy: 0.7467\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7357 - accuracy: 0.7467\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7293 - accuracy: 0.7467\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7231 - accuracy: 0.7467\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7171 - accuracy: 0.7467\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7112 - accuracy: 0.7467\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7054 - accuracy: 0.7467\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6998 - accuracy: 0.7467\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6944 - accuracy: 0.7533\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6890 - accuracy: 0.7533\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6837 - accuracy: 0.7533\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6786 - accuracy: 0.7533\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6735 - accuracy: 0.7533\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6685 - accuracy: 0.7533\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6635 - accuracy: 0.7533\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6586 - accuracy: 0.7600\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6538 - accuracy: 0.7733\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6490 - accuracy: 0.7867\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6442 - accuracy: 0.7867\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6396 - accuracy: 0.7933\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6349 - accuracy: 0.7933\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6302 - accuracy: 0.7933\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6256 - accuracy: 0.8000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6211 - accuracy: 0.8000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6168 - accuracy: 0.8000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6127 - accuracy: 0.8067\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6088 - accuracy: 0.8067\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6049 - accuracy: 0.8067\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6012 - accuracy: 0.8067\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5975 - accuracy: 0.8133\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5940 - accuracy: 0.8133\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5906 - accuracy: 0.8133\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5872 - accuracy: 0.8133\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5839 - accuracy: 0.8133\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5807 - accuracy: 0.8133\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5776 - accuracy: 0.8133\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5745 - accuracy: 0.8267\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5714 - accuracy: 0.8333\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5684 - accuracy: 0.8333\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5655 - accuracy: 0.8400\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5626 - accuracy: 0.8533\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5597 - accuracy: 0.8533\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5569 - accuracy: 0.8533\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5542 - accuracy: 0.8533\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5515 - accuracy: 0.8533\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5488 - accuracy: 0.8533\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5462 - accuracy: 0.8533\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5436 - accuracy: 0.8533\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5410 - accuracy: 0.8533\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5385 - accuracy: 0.8533\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5361 - accuracy: 0.8533\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.5336 - accuracy: 0.8600\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5312 - accuracy: 0.8667\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5289 - accuracy: 0.8733\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5265 - accuracy: 0.8733\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5242 - accuracy: 0.8733\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5220 - accuracy: 0.8733\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5198 - accuracy: 0.8733\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5176 - accuracy: 0.8733\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5154 - accuracy: 0.8733\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5132 - accuracy: 0.8733\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5111 - accuracy: 0.8733\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5091 - accuracy: 0.8733\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5070 - accuracy: 0.8733\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5050 - accuracy: 0.8733\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5030 - accuracy: 0.8733\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5010 - accuracy: 0.8733\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4991 - accuracy: 0.8733\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4971 - accuracy: 0.8733\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4952 - accuracy: 0.8800\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4933 - accuracy: 0.8800\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4914 - accuracy: 0.8800\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4896 - accuracy: 0.8800\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4878 - accuracy: 0.8867\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4860 - accuracy: 0.8867\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4842 - accuracy: 0.8867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d969bdfcd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = load_iris()['data']\n",
    "Y = make_onehot(load_iris()['target'])\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(100, activation='relu', input_shape=(4, )))\n",
    "model.add(keras.layers.Dense(50, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# %%time\n",
    "model.fit(X, Y, epochs=100, batch_size=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 10.745397068379418 0.3333333333333333\n",
      "2 10.745397068379416 0.3333333333333333\n",
      "3 10.745397068379416 0.3333333333333333\n",
      "4 10.745397068379416 0.3333333333333333\n",
      "5 10.745397068379415 0.3333333333333333\n",
      "6 10.745397068379415 0.3333333333333333\n",
      "7 10.745397068379413 0.3333333333333333\n",
      "8 10.745397068379413 0.3333333333333333\n",
      "9 10.745397068379413 0.3333333333333333\n",
      "10 10.745397068379411 0.3333333333333333\n",
      "11 10.745397068379411 0.3333333333333333\n",
      "12 10.74539706837941 0.3333333333333333\n",
      "13 10.74539706837941 0.3333333333333333\n",
      "14 10.74539706837941 0.3333333333333333\n",
      "15 10.745397068379408 0.3333333333333333\n",
      "16 10.745397068379408 0.3333333333333333\n",
      "17 10.745397068379408 0.3333333333333333\n",
      "18 10.745397068379408 0.3333333333333333\n",
      "19 10.745397068379408 0.3333333333333333\n",
      "20 10.745397068379406 0.3333333333333333\n",
      "21 10.745397068379406 0.3333333333333333\n",
      "22 10.745397068379404 0.3333333333333333\n",
      "23 10.745397068379404 0.3333333333333333\n",
      "24 10.745397068379404 0.3333333333333333\n",
      "25 10.745397068379402 0.3333333333333333\n",
      "26 10.745397068379402 0.3333333333333333\n",
      "27 10.7453970683794 0.3333333333333333\n",
      "28 10.7453970683794 0.3333333333333333\n",
      "29 10.7453970683794 0.3333333333333333\n",
      "30 10.745397068379399 0.3333333333333333\n",
      "31 10.745397068379399 0.3333333333333333\n",
      "32 10.745397068379397 0.3333333333333333\n",
      "33 10.745397068379397 0.3333333333333333\n",
      "34 10.745397068379397 0.3333333333333333\n",
      "35 10.745397068379397 0.3333333333333333\n",
      "36 10.745397068379397 0.3333333333333333\n",
      "37 10.745397068379395 0.3333333333333333\n",
      "38 10.745397068379395 0.3333333333333333\n",
      "39 10.745397068379395 0.3333333333333333\n",
      "40 10.745397068379393 0.3333333333333333\n",
      "41 10.745397068379393 0.3333333333333333\n",
      "42 10.745397068379392 0.3333333333333333\n",
      "43 10.745397068379392 0.3333333333333333\n",
      "44 10.745397068379392 0.3333333333333333\n",
      "45 10.74539706837939 0.3333333333333333\n",
      "46 10.74539706837939 0.3333333333333333\n",
      "47 10.745397068379388 0.3333333333333333\n",
      "48 10.745397068379388 0.3333333333333333\n",
      "49 10.745397068379388 0.3333333333333333\n",
      "50 10.745397068379388 0.3333333333333333\n",
      "51 10.745397068379388 0.3333333333333333\n",
      "52 10.745397068379386 0.3333333333333333\n",
      "53 10.745397068379386 0.3333333333333333\n",
      "54 10.745397068379386 0.3333333333333333\n",
      "55 10.745397068379384 0.3333333333333333\n",
      "56 10.745397068379384 0.3333333333333333\n",
      "57 10.745397068379383 0.3333333333333333\n",
      "58 10.745397068379383 0.3333333333333333\n",
      "59 10.745397068379383 0.3333333333333333\n",
      "60 10.745397068379381 0.3333333333333333\n",
      "61 10.745397068379381 0.3333333333333333\n",
      "62 10.74539706837938 0.3333333333333333\n",
      "63 10.74539706837938 0.3333333333333333\n",
      "64 10.74539706837938 0.3333333333333333\n",
      "65 10.745397068379377 0.3333333333333333\n",
      "66 10.745397068379377 0.3333333333333333\n",
      "67 10.745397068379377 0.3333333333333333\n",
      "68 10.745397068379377 0.3333333333333333\n",
      "69 10.745397068379377 0.3333333333333333\n",
      "70 10.745397068379376 0.3333333333333333\n",
      "71 10.745397068379374 0.3333333333333333\n",
      "72 10.745397068379374 0.3333333333333333\n",
      "73 10.745397068379374 0.3333333333333333\n",
      "74 10.745397068379374 0.3333333333333333\n",
      "75 10.745397068379372 0.3333333333333333\n",
      "76 10.74539706837937 0.3333333333333333\n",
      "77 10.74539706837937 0.3333333333333333\n",
      "78 10.74539706837937 0.3333333333333333\n",
      "79 10.74539706837937 0.3333333333333333\n",
      "80 10.745397068379368 0.3333333333333333\n",
      "81 10.745397068379367 0.3333333333333333\n",
      "82 10.745397068379367 0.3333333333333333\n",
      "83 10.745397068379367 0.3333333333333333\n",
      "84 10.745397068379367 0.3333333333333333\n",
      "85 10.745397068379367 0.3333333333333333\n",
      "86 10.745397068379365 0.3333333333333333\n",
      "87 10.745397068379365 0.3333333333333333\n",
      "88 10.745397068379365 0.3333333333333333\n",
      "89 10.745397068379363 0.3333333333333333\n",
      "90 10.745397068379363 0.3333333333333333\n",
      "91 10.745397068379361 0.3333333333333333\n",
      "92 10.745397068379361 0.3333333333333333\n",
      "93 10.745397068379361 0.3333333333333333\n",
      "94 10.74539706837936 0.3333333333333333\n",
      "95 10.74539706837936 0.3333333333333333\n",
      "96 10.745397068379358 0.3333333333333333\n",
      "97 10.745397068379358 0.3333333333333333\n",
      "98 10.745397068379358 0.3333333333333333\n",
      "99 10.745397068379356 0.3333333333333333\n",
      "100 10.745397068379356 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "X = load_iris()['data']\n",
    "Y = make_onehot(load_iris()['target'])\n",
    "\n",
    "# %%time\n",
    "model2 = Network()\n",
    "\n",
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    e += 1\n",
    "    model2.gradient(X, Y, 1e-3)\n",
    "    print(e, model2.loss(X, Y), model2.accuracy(X, Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multilayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    def forward(self, X, Y):\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "        out = self.x * self.y\n",
    "        return out\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.x\n",
    "        dy = dout * self.y\n",
    "        return dx, dy\n",
    "\n",
    "class Addlayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    def forward(self, X, Y):\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "        out = self.x * self.y\n",
    "        return out\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    def forward(self, X):\n",
    "        self.mask = (x <= 0)\n",
    "        out = X.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx\n",
    "        \n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    def forward(self, X):\n",
    "        out = Activation.sigmoid(x)\n",
    "        self.out = out\n",
    "        return self.out\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1. - self.out)\n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.x = None\n",
    "        self.origin_shape = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "    def forward(self, x):\n",
    "        self.origin_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "        out = np.dot(self.x, self.w) + self.b\n",
    "        return out\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.w.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        dx = dx.reshape(self.origin_shape)\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = Activation.softmax(x)\n",
    "        self.loss = cross_entropy(self.y, self.t)\n",
    "        return self.loss\n",
    "    def backward(self, dout=1):\n",
    "        dx = self.y - self.t\n",
    "        return dx\n",
    "\n",
    "class LeakyRelu:\n",
    "    def __init__(self, alpha = 0.01):\n",
    "        self.alpha = alpha\n",
    "    def forward(self, x):\n",
    "        out = np.where(x > 0, x, self.alpha * x)\n",
    "        return out\n",
    "    def backward(self, dout):\n",
    "        dx = np.where(dout > 0 , dout, self.alpha * dout)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.38158260e-03, -4.36348796e-04, -2.07241186e-03,\n",
       "        -1.29543984e-03],\n",
       "       [-4.64023584e-04, -5.70266371e-05,  2.75929737e-01,\n",
       "         1.49451522e+00],\n",
       "       [ 5.66820637e-01, -2.30337426e-03,  1.37700072e+00,\n",
       "        -1.18738682e-03]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mul = Multilayer()\n",
    "mul.forward(100, 2)\n",
    "mul.backward(100)\n",
    "\n",
    "add = Addlayer()\n",
    "add.forward(100, 2)\n",
    "# add.backward(100)\n",
    "\n",
    "np.random.seed(100)\n",
    "x = np.random.randint(-5, 50, 30)\n",
    "x = x.reshape(5, 6)\n",
    "\n",
    "act = LeakyRelu(0.001)\n",
    "act.forward(np.random.randn(3, 4))\n",
    "act.backward(np.random.randn(3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5/5 [==============================] - 1s 15ms/step - loss: 1.1385 - accuracy: 0.3933\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.9527 - accuracy: 0.6867\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.8817 - accuracy: 0.6667\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.8537 - accuracy: 0.6933\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.8122 - accuracy: 0.7133\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7471 - accuracy: 0.6867\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7060 - accuracy: 0.7800\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6850 - accuracy: 0.7333\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6727 - accuracy: 0.7667\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6167 - accuracy: 0.7467\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5956 - accuracy: 0.7800\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5917 - accuracy: 0.7333\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5786 - accuracy: 0.8133\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5427 - accuracy: 0.8267\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5227 - accuracy: 0.8533\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5155 - accuracy: 0.8133\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5072 - accuracy: 0.8133\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4807 - accuracy: 0.9267\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4872 - accuracy: 0.8733\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4689 - accuracy: 0.8867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d97e87d370>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, Y = load_iris()['data'], load_iris()['target']\n",
    "Y = to_categorical(Y)\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(4, )))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, Y, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_TF_MODULE_IGNORED_PROPERTIES',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activity_regularizer',\n",
       " '_add_trackable',\n",
       " '_add_variable_with_custom_getter',\n",
       " '_auto_track_sub_layers',\n",
       " '_autocast',\n",
       " '_autographed_call',\n",
       " '_batch_input_shape',\n",
       " '_build_input_shape',\n",
       " '_call_accepts_kwargs',\n",
       " '_call_arg_was_passed',\n",
       " '_call_fn_arg_defaults',\n",
       " '_call_fn_arg_positions',\n",
       " '_call_fn_args',\n",
       " '_call_full_argspec',\n",
       " '_callable_losses',\n",
       " '_cast_single_input',\n",
       " '_checkpoint_dependencies',\n",
       " '_clear_losses',\n",
       " '_compute_dtype',\n",
       " '_compute_dtype_object',\n",
       " '_dedup_weights',\n",
       " '_default_training_arg',\n",
       " '_deferred_dependencies',\n",
       " '_delete_tracking',\n",
       " '_dtype',\n",
       " '_dtype_policy',\n",
       " '_dynamic',\n",
       " '_eager_losses',\n",
       " '_expects_mask_arg',\n",
       " '_expects_training_arg',\n",
       " '_flatten',\n",
       " '_flatten_layers',\n",
       " '_flatten_modules',\n",
       " '_functional_construction_call',\n",
       " '_gather_children_attribute',\n",
       " '_gather_saveables_for_checkpoint',\n",
       " '_get_call_arg_value',\n",
       " '_get_cell_name',\n",
       " '_get_existing_metric',\n",
       " '_get_input_masks',\n",
       " '_get_node_attribute_at_index',\n",
       " '_get_save_spec',\n",
       " '_get_trainable_state',\n",
       " '_handle_activity_regularization',\n",
       " '_handle_deferred_dependencies',\n",
       " '_handle_weight_regularization',\n",
       " '_inbound_nodes',\n",
       " '_inbound_nodes_value',\n",
       " '_infer_output_signature',\n",
       " '_init_call_fn_args',\n",
       " '_init_set_name',\n",
       " '_initial_weights',\n",
       " '_input_spec',\n",
       " '_instrument_layer_creation',\n",
       " '_instrumented_keras_api',\n",
       " '_instrumented_keras_layer_class',\n",
       " '_instrumented_keras_model_class',\n",
       " '_is_layer',\n",
       " '_keras_api_names',\n",
       " '_keras_api_names_v1',\n",
       " '_keras_tensor_symbolic_call',\n",
       " '_list_extra_dependencies_for_serialization',\n",
       " '_list_functions_for_serialization',\n",
       " '_lookup_dependency',\n",
       " '_losses',\n",
       " '_map_resources',\n",
       " '_maybe_build',\n",
       " '_maybe_cast_inputs',\n",
       " '_maybe_create_attribute',\n",
       " '_maybe_initialize_trackable',\n",
       " '_metrics',\n",
       " '_metrics_lock',\n",
       " '_must_restore_from_config',\n",
       " '_name',\n",
       " '_name_based_attribute_restore',\n",
       " '_name_based_restores',\n",
       " '_name_scope',\n",
       " '_no_dependency',\n",
       " '_non_trainable_weights',\n",
       " '_obj_reference_counts',\n",
       " '_obj_reference_counts_dict',\n",
       " '_object_identifier',\n",
       " '_outbound_nodes',\n",
       " '_outbound_nodes_value',\n",
       " '_preload_simple_restoration',\n",
       " '_preserve_input_structure_in_config',\n",
       " '_restore_from_checkpoint_position',\n",
       " '_saved_model_inputs_spec',\n",
       " '_self_name_based_restores',\n",
       " '_self_saveable_object_factories',\n",
       " '_self_setattr_tracking',\n",
       " '_self_tracked_trackables',\n",
       " '_self_unconditional_checkpoint_dependencies',\n",
       " '_self_unconditional_deferred_dependencies',\n",
       " '_self_unconditional_dependency_names',\n",
       " '_self_update_uid',\n",
       " '_set_call_arg_value',\n",
       " '_set_connectivity_metadata',\n",
       " '_set_dtype_policy',\n",
       " '_set_mask_keras_history_checked',\n",
       " '_set_mask_metadata',\n",
       " '_set_save_spec',\n",
       " '_set_trainable_state',\n",
       " '_set_training_mode',\n",
       " '_setattr_tracking',\n",
       " '_should_cast_single_input',\n",
       " '_single_restoration_from_checkpoint_position',\n",
       " '_split_out_first_arg',\n",
       " '_stateful',\n",
       " '_supports_masking',\n",
       " '_symbolic_call',\n",
       " '_tf_api_names',\n",
       " '_tf_api_names_v1',\n",
       " '_thread_local',\n",
       " '_track_trackable',\n",
       " '_trackable_saved_model_saver',\n",
       " '_tracking_metadata',\n",
       " '_trainable',\n",
       " '_trainable_weights',\n",
       " '_unconditional_checkpoint_dependencies',\n",
       " '_unconditional_dependency_names',\n",
       " '_update_uid',\n",
       " '_updates',\n",
       " '_use_input_spec_as_call_signature',\n",
       " 'activation',\n",
       " 'activity_regularizer',\n",
       " 'add_loss',\n",
       " 'add_metric',\n",
       " 'add_update',\n",
       " 'add_variable',\n",
       " 'add_weight',\n",
       " 'apply',\n",
       " 'bias',\n",
       " 'bias_constraint',\n",
       " 'bias_initializer',\n",
       " 'bias_regularizer',\n",
       " 'build',\n",
       " 'built',\n",
       " 'call',\n",
       " 'compute_dtype',\n",
       " 'compute_mask',\n",
       " 'compute_output_shape',\n",
       " 'compute_output_signature',\n",
       " 'count_params',\n",
       " 'dtype',\n",
       " 'dtype_policy',\n",
       " 'dynamic',\n",
       " 'from_config',\n",
       " 'get_config',\n",
       " 'get_input_at',\n",
       " 'get_input_mask_at',\n",
       " 'get_input_shape_at',\n",
       " 'get_losses_for',\n",
       " 'get_output_at',\n",
       " 'get_output_mask_at',\n",
       " 'get_output_shape_at',\n",
       " 'get_updates_for',\n",
       " 'get_weights',\n",
       " 'inbound_nodes',\n",
       " 'input',\n",
       " 'input_mask',\n",
       " 'input_shape',\n",
       " 'input_spec',\n",
       " 'kernel',\n",
       " 'kernel_constraint',\n",
       " 'kernel_initializer',\n",
       " 'kernel_regularizer',\n",
       " 'losses',\n",
       " 'metrics',\n",
       " 'name',\n",
       " 'name_scope',\n",
       " 'non_trainable_variables',\n",
       " 'non_trainable_weights',\n",
       " 'outbound_nodes',\n",
       " 'output',\n",
       " 'output_mask',\n",
       " 'output_shape',\n",
       " 'set_weights',\n",
       " 'stateful',\n",
       " 'submodules',\n",
       " 'supports_masking',\n",
       " 'trainable',\n",
       " 'trainable_variables',\n",
       " 'trainable_weights',\n",
       " 'units',\n",
       " 'updates',\n",
       " 'use_bias',\n",
       " 'variable_dtype',\n",
       " 'variables',\n",
       " 'weights',\n",
       " 'with_name_scope']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class MultiNet:\n",
    "    def __init__(self):\n",
    "        self.w = {\n",
    "            'w1': np.random.randn(4, 100),\n",
    "            'w2': np.random.randn(100, 50),\n",
    "            'w3': np.random.randn(50, 3),\n",
    "            'b1': np.random.randn(100),\n",
    "            'b2': np.random.randn(50),\n",
    "            'b3': np.random.randn(3),\n",
    "        }\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.w['w1'], self.w['b1'])\n",
    "        self.layers['relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.w['w2'], self.w['b2'])\n",
    "        self.layers['relu2'] = Relu()\n",
    "        self.layers['Affine3'] = Affine(self.w['w3'], self.w['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for i in range(len(self.w)-2):\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('hrd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c2e67704c6d152f142b8c3106687cd2194e93bf9cff81b34c8c3689f569f836"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
