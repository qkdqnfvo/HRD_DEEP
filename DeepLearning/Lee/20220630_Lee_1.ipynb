{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "from Activations import Activation\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data x1,x2\n",
    "import numpy as np\n",
    "x1 = np.random.randint(50,80,10)\n",
    "x2 = np.random.randint(160,180,10)\n",
    "X = np.c_[x1,x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.random.randn(1)\n",
    "w2 = np.random.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_w1 = w1*x1\n",
    "out_w2 = w2*x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-120.32044157,  213.27413754],\n",
       "       [-114.20245302,  210.80854057],\n",
       "       [-157.0283729 ,  198.48055575],\n",
       "       [-152.94971386,  203.41174968],\n",
       "       [-108.08446446,  213.27413754],\n",
       "       [-146.83172531,  200.94615271],\n",
       "       [-122.35977109,  198.48055575],\n",
       "       [-146.83172531,  207.11014513],\n",
       "       [-134.5957482 ,  210.80854057],\n",
       "       [-144.79239579,  218.20533147]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = np.c_[out_w1,out_w2]\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.23279848])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [0],\n",
       "       [1],\n",
       "       [2]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_hot -- keras.utils.to_categorical(data)\n",
    "y = np.random.randint(0,5,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data\n",
    "def make_onehot(x):\n",
    "    col = np.unique(x).size\n",
    "    row = x.size\n",
    "    data = np.zeros((row,col))\n",
    "    for i in np.arange(row):\n",
    "        data[i,x[i]] = 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "from sklearn.datasets import load_iris\n",
    "X = load_iris()['data']\n",
    "y = load_iris()['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(4,1)\n",
    "b = np.random.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.dot(X,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## y onehot\n",
    "y = make_onehot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(4,3)\n",
    "b = np.random.randn(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91813414, 0.00923423, 0.07263163],\n",
       "       [0.82588953, 0.03173677, 0.1423737 ],\n",
       "       [0.88113627, 0.01568035, 0.10318338],\n",
       "       [0.80247259, 0.01799778, 0.17952963],\n",
       "       [0.92593924, 0.00635653, 0.06770422],\n",
       "       [0.94514998, 0.00520696, 0.04964306],\n",
       "       [0.90223823, 0.00966847, 0.0880933 ],\n",
       "       [0.88279328, 0.01112281, 0.10608391],\n",
       "       [0.77275549, 0.0267121 , 0.2005324 ],\n",
       "       [0.79650449, 0.01924756, 0.18424795],\n",
       "       [0.93316221, 0.0066051 , 0.06023269],\n",
       "       [0.84781913, 0.00913405, 0.14304682],\n",
       "       [0.79868297, 0.02388873, 0.17742831],\n",
       "       [0.8571773 , 0.01676943, 0.12605328],\n",
       "       [0.97911213, 0.00382364, 0.01706424],\n",
       "       [0.98428367, 0.00152101, 0.01419532],\n",
       "       [0.97430608, 0.00523985, 0.02045406],\n",
       "       [0.92869978, 0.01106335, 0.06023687],\n",
       "       [0.93449325, 0.00749221, 0.05801454],\n",
       "       [0.94386481, 0.00459073, 0.05154446],\n",
       "       [0.85458297, 0.01508553, 0.13033149],\n",
       "       [0.94431822, 0.00734783, 0.04833396],\n",
       "       [0.96036977, 0.00464939, 0.03498085],\n",
       "       [0.87382523, 0.02712798, 0.09904679],\n",
       "       [0.74260276, 0.00814633, 0.24925091],\n",
       "       [0.76907921, 0.03244494, 0.19847584],\n",
       "       [0.89334931, 0.01588641, 0.09076428],\n",
       "       [0.90486421, 0.00993112, 0.08520467],\n",
       "       [0.90882514, 0.01339157, 0.0777833 ],\n",
       "       [0.79931266, 0.01448353, 0.18620381],\n",
       "       [0.78211741, 0.02076281, 0.19711978],\n",
       "       [0.9180323 , 0.02246303, 0.05950467],\n",
       "       [0.94967197, 0.00144932, 0.04887871],\n",
       "       [0.97410626, 0.00165372, 0.02424001],\n",
       "       [0.82082604, 0.02349392, 0.15568004],\n",
       "       [0.90784463, 0.02049391, 0.07166146],\n",
       "       [0.93957834, 0.01300269, 0.04741897],\n",
       "       [0.90944637, 0.00485951, 0.08569412],\n",
       "       [0.82922866, 0.02109473, 0.14967661],\n",
       "       [0.88699777, 0.01212225, 0.10087998],\n",
       "       [0.93860762, 0.01024649, 0.05114589],\n",
       "       [0.61527051, 0.16491009, 0.2198194 ],\n",
       "       [0.8679989 , 0.01210365, 0.11989746],\n",
       "       [0.92745423, 0.01713087, 0.05541489],\n",
       "       [0.8945137 , 0.00527881, 0.10020748],\n",
       "       [0.83941625, 0.03522328, 0.12536047],\n",
       "       [0.91920366, 0.00379736, 0.07699897],\n",
       "       [0.85315695, 0.01408158, 0.13276147],\n",
       "       [0.93047028, 0.00607183, 0.06345789],\n",
       "       [0.88798954, 0.01502103, 0.09698942],\n",
       "       [0.10548762, 0.1139324 , 0.78057998],\n",
       "       [0.14058685, 0.10910537, 0.75030778],\n",
       "       [0.07040608, 0.11351389, 0.81608003],\n",
       "       [0.05357427, 0.20699692, 0.73942881],\n",
       "       [0.06818428, 0.19218136, 0.73963436],\n",
       "       [0.0455134 , 0.04743247, 0.90705414],\n",
       "       [0.12449331, 0.07905655, 0.79645014],\n",
       "       [0.12289451, 0.12453795, 0.75256754],\n",
       "       [0.06260532, 0.10100858, 0.8363861 ],\n",
       "       [0.12067626, 0.12924403, 0.75007971],\n",
       "       [0.04912457, 0.18188988, 0.76898555],\n",
       "       [0.15284424, 0.14154095, 0.70561481],\n",
       "       [0.03642695, 0.17177387, 0.79179917],\n",
       "       [0.04896715, 0.06269859, 0.88833426],\n",
       "       [0.24914712, 0.16788068, 0.5829722 ],\n",
       "       [0.14008931, 0.1572684 , 0.70264228],\n",
       "       [0.07954285, 0.05877107, 0.86168608],\n",
       "       [0.05703784, 0.05116174, 0.89180042],\n",
       "       [0.0270046 , 0.35994988, 0.61304553],\n",
       "       [0.06919942, 0.11263252, 0.81816807],\n",
       "       [0.10437586, 0.09127908, 0.80434505],\n",
       "       [0.13365985, 0.18710406, 0.67923609],\n",
       "       [0.0232575 , 0.13979082, 0.83695168],\n",
       "       [0.0302288 , 0.03726416, 0.93250704],\n",
       "       [0.10158378, 0.1368094 , 0.76160682],\n",
       "       [0.11825985, 0.16531791, 0.71642224],\n",
       "       [0.04584073, 0.14089879, 0.81326047],\n",
       "       [0.06203918, 0.16206702, 0.7758938 ],\n",
       "       [0.08118586, 0.11215406, 0.80666009],\n",
       "       [0.14600112, 0.15728759, 0.69671128],\n",
       "       [0.06959292, 0.14020308, 0.790204  ],\n",
       "       [0.0729822 , 0.1233893 , 0.8036285 ],\n",
       "       [0.11250562, 0.13988149, 0.74761289],\n",
       "       [0.02163404, 0.06695481, 0.91141116],\n",
       "       [0.07287661, 0.04576571, 0.88135768],\n",
       "       [0.1805855 , 0.06573214, 0.75368236],\n",
       "       [0.09492303, 0.12851979, 0.77655718],\n",
       "       [0.03245204, 0.24612254, 0.72142542],\n",
       "       [0.12570835, 0.06462978, 0.80966187],\n",
       "       [0.07309929, 0.15481683, 0.77208387],\n",
       "       [0.03273265, 0.04438927, 0.92287807],\n",
       "       [0.06867743, 0.06471435, 0.86660821],\n",
       "       [0.08167906, 0.13799492, 0.78032603],\n",
       "       [0.10977115, 0.16297238, 0.72725648],\n",
       "       [0.06956688, 0.08866252, 0.84177059],\n",
       "       [0.0923863 , 0.04376021, 0.86385349],\n",
       "       [0.09504246, 0.07202078, 0.83293676],\n",
       "       [0.09510571, 0.10886487, 0.79602942],\n",
       "       [0.24971573, 0.2564166 , 0.49386767],\n",
       "       [0.10017031, 0.10190968, 0.79792002],\n",
       "       [0.03960053, 0.12477865, 0.83562082],\n",
       "       [0.03195437, 0.13967378, 0.82837185],\n",
       "       [0.02261656, 0.16993337, 0.80745007],\n",
       "       [0.01614331, 0.05053109, 0.9333256 ],\n",
       "       [0.02576258, 0.13993047, 0.83430695],\n",
       "       [0.00677032, 0.0796696 , 0.91356008],\n",
       "       [0.04075652, 0.10750127, 0.85174222],\n",
       "       [0.00573736, 0.0422314 , 0.95203124],\n",
       "       [0.00709786, 0.10359763, 0.8893045 ],\n",
       "       [0.06897151, 0.18442376, 0.74660472],\n",
       "       [0.09577197, 0.19485866, 0.70936938],\n",
       "       [0.0265425 , 0.19124391, 0.78221359],\n",
       "       [0.0424095 , 0.24374723, 0.71384327],\n",
       "       [0.03031274, 0.26236698, 0.70732028],\n",
       "       [0.0590063 , 0.44516013, 0.49583357],\n",
       "       [0.0910931 , 0.28737061, 0.62153629],\n",
       "       [0.02516879, 0.06821384, 0.90661737],\n",
       "       [0.02103583, 0.02888481, 0.95007936],\n",
       "       [0.00283032, 0.17175438, 0.8254153 ],\n",
       "       [0.01081992, 0.12632612, 0.86285396],\n",
       "       [0.05314763, 0.25788548, 0.68896689],\n",
       "       [0.05766233, 0.18561024, 0.75672743],\n",
       "       [0.00361323, 0.07146366, 0.9249231 ],\n",
       "       [0.04618084, 0.25282818, 0.70099099],\n",
       "       [0.04543667, 0.09889034, 0.85567299],\n",
       "       [0.01594754, 0.0431333 , 0.94091915],\n",
       "       [0.06301155, 0.23405199, 0.70293646],\n",
       "       [0.07025988, 0.13268037, 0.79705975],\n",
       "       [0.02320331, 0.17681673, 0.79997996],\n",
       "       [0.01299016, 0.04514112, 0.94186872],\n",
       "       [0.00937394, 0.11830116, 0.8723249 ],\n",
       "       [0.03088111, 0.03492483, 0.93419406],\n",
       "       [0.02611376, 0.23570006, 0.73818618],\n",
       "       [0.02405314, 0.05938317, 0.91656369],\n",
       "       [0.00459411, 0.01530184, 0.98010404],\n",
       "       [0.02190092, 0.3805482 , 0.59755088],\n",
       "       [0.0838838 , 0.16128317, 0.75483303],\n",
       "       [0.02766662, 0.05118111, 0.92115227],\n",
       "       [0.08155813, 0.14113877, 0.77730309],\n",
       "       [0.05999283, 0.27524204, 0.66476513],\n",
       "       [0.05451579, 0.35752636, 0.58795785],\n",
       "       [0.09036846, 0.57124327, 0.33838827],\n",
       "       [0.03195437, 0.13967378, 0.82837185],\n",
       "       [0.03677468, 0.16650092, 0.79672441],\n",
       "       [0.07249282, 0.31053463, 0.61697255],\n",
       "       [0.06987795, 0.51015282, 0.41996923],\n",
       "       [0.02998027, 0.35680742, 0.61321231],\n",
       "       [0.05914698, 0.22086736, 0.71998566],\n",
       "       [0.10193831, 0.1507288 , 0.74733289],\n",
       "       [0.04413637, 0.07169939, 0.88416424]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Activation.softmax(np.dot(X,W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3])\n",
    "np.exp(x)/np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y,t):\n",
    "    return np.sum((y-t)**2)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.8376458028469065"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.random.randn(10)\n",
    "t = np.random.randn(10)\n",
    "mse(y,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y,t):\n",
    "    epsilon = 1e-7\n",
    "    y = Activation.softmax(y)\n",
    "    return -np.sum(t*np.log(y+epsilon))/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1,7,0])\n",
    "t = np.array([0,1,0])\n",
    "x1  =cross_entropy(y,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([2,4,6])\n",
    "t = np.array([0,0,1])\n",
    "x2 = cross_entropy(y,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def der(f,x):\n",
    "    h = 1e-7\n",
    "    fxh = f(x+h)\n",
    "    fx = f(x)\n",
    "    return (fxh - fx)/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x:x**2 +2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff(x,y):\n",
    "    return x**2 + y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.00000009348878"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "der(함수,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fx(x):\n",
    "    return x[0]**2+x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def par_der(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x,flags=['multi_index'],op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "        x[idx] = float(tmp_val) - h\n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1 - fxh2)/(2*h)\n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([3.,4.])\n",
    "par_der(fx,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(5,6)\n",
    "it = np.nditer(x,flags=['multi_index'],op_flags=['readwrite'])\n",
    "while not it.finished:\n",
    "    x[it.multi_index]\n",
    "    it.iternext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x,flags=['multi_index'],op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1-fxh2)/(2*h)\n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2d(x):\n",
    "    return x[0]**2+x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([2.,3.])\n",
    "function_2d(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 6.])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2d,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_iris()['data']\n",
    "y = load_iris()['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = make_onehot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(4,10)\n",
    "b1 = np.random.randn(10)\n",
    "W2 = np.random.randn(10,5)\n",
    "b2 = np.random.randn(5)\n",
    "W3 = np.random.randn(5,3)\n",
    "b3 = np.random.randn(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = np.dot(X,W1) + b1\n",
    "z1 = Activation.relu(layer1)\n",
    "layer2 = np.dot(z1,W2) + b2\n",
    "z2 = Activation.relu(layer2)\n",
    "layer3 = np.dot(z2,W3) + b3\n",
    "output = Activation.softmax(layer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross_entropy(y,output)\n",
    "loss = cross_entropy\n",
    "f = lambda w : loss(y,output)\n",
    "dw = numerical_gradient(f,W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.W = {\n",
    "            'W1':np.random.randn(4,100),\n",
    "            'W2':np.random.randn(100,50),\n",
    "            'W3':np.random.randn(50,3)\n",
    "        }\n",
    "        self.b = {\n",
    "            'b1':np.random.randn(100),\n",
    "            'b2':np.random.randn(50),\n",
    "            'b3':np.random.randn(3)\n",
    "        }\n",
    "        self.Activation ={\n",
    "            'sigmoid':Activation.sigmoid,\n",
    "            'relu':Activation.relu,\n",
    "            'softmax':Activation.softmax\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def predict(self,x):\n",
    "        result = np.dot(x,self.W['W1']) + self.b['b1']\n",
    "        for i in range(len(self.W)-2):\n",
    "            result = np.dot(result,self.W['W'+str(i+2)]) + self.b['b'+str(i+2)]\n",
    "            result = Activation.relu(result)    \n",
    "        result = np.dot(result,self.W['W'+str(len(self.W))]) + self.b['b'+str(len(self.W))]    \n",
    "        return Activation.softmax(result)\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        self.y = self.predict(x)\n",
    "        loss = cross_entropy(self.y,t)\n",
    "        return loss\n",
    "    \n",
    "    def gradient(self,x,t,learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        f = lambda w : self.loss(x,t)\n",
    "        for i in range(len(self.W)):\n",
    "            self.W['W'+str(i+1)] -= self.learning_rate*numerical_gradient(f,self.W['W'+str(i+1)])\n",
    "            self.b['b'+str(i+1)] -= self.learning_rate*numerical_gradient(f,self.b['b'+str(i+1)])\n",
    "            \n",
    "\n",
    "        \n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        result = self.predict(x)\n",
    "        acc = sum(np.argmax(result,axis=1) == np.argmax(t,axis=1))/len(t)\n",
    "        return acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "X = load_iris()['data']\n",
    "y = load_iris()['target']\n",
    "y = make_onehot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss==== 0.8400005474907304 accuracy==== 0.7133333333333334\n",
      "epoch 2 loss==== 0.8326060947933777 accuracy==== 0.72\n",
      "epoch 3 loss==== 0.8270238572989062 accuracy==== 0.72\n",
      "epoch 4 loss==== 0.8252993445740644 accuracy==== 0.72\n",
      "epoch 5 loss==== 0.8250360592190409 accuracy==== 0.7266666666666667\n",
      "epoch 6 loss==== 0.824935449360947 accuracy==== 0.7266666666666667\n",
      "epoch 7 loss==== 0.8248721279651884 accuracy==== 0.7266666666666667\n",
      "epoch 8 loss==== 0.824820061172984 accuracy==== 0.7266666666666667\n",
      "epoch 9 loss==== 0.8247706801525538 accuracy==== 0.7266666666666667\n",
      "epoch 10 loss==== 0.8247203724387024 accuracy==== 0.7266666666666667\n",
      "epoch 11 loss==== 0.8246674264931328 accuracy==== 0.7266666666666667\n",
      "epoch 12 loss==== 0.8246110605448603 accuracy==== 0.7266666666666667\n",
      "epoch 13 loss==== 0.8245510382175042 accuracy==== 0.7266666666666667\n",
      "epoch 14 loss==== 0.8244874607966935 accuracy==== 0.7266666666666667\n",
      "epoch 15 loss==== 0.8244206169937848 accuracy==== 0.7266666666666667\n",
      "epoch 16 loss==== 0.8243508710267323 accuracy==== 0.7266666666666667\n",
      "epoch 17 loss==== 0.8242785942833871 accuracy==== 0.7266666666666667\n",
      "epoch 18 loss==== 0.8242041360968837 accuracy==== 0.7266666666666667\n",
      "epoch 19 loss==== 0.8241278177626729 accuracy==== 0.72\n",
      "epoch 20 loss==== 0.8240499347303419 accuracy==== 0.72\n",
      "epoch 21 loss==== 0.8239707594126234 accuracy==== 0.72\n",
      "epoch 22 loss==== 0.8238905428982995 accuracy==== 0.72\n",
      "epoch 23 loss==== 0.8238095159780715 accuracy==== 0.7266666666666667\n",
      "epoch 24 loss==== 0.8237278900127992 accuracy==== 0.7266666666666667\n",
      "epoch 25 loss==== 0.8236458578672314 accuracy==== 0.7266666666666667\n",
      "epoch 26 loss==== 0.8235635949373697 accuracy==== 0.7266666666666667\n",
      "epoch 27 loss==== 0.8234812602354589 accuracy==== 0.7266666666666667\n",
      "epoch 28 loss==== 0.8233989974911039 accuracy==== 0.7266666666666667\n",
      "epoch 29 loss==== 0.8233169362372084 accuracy==== 0.7266666666666667\n",
      "epoch 30 loss==== 0.8232351928602969 accuracy==== 0.7266666666666667\n",
      "epoch 31 loss==== 0.8231538716029716 accuracy==== 0.7266666666666667\n",
      "epoch 32 loss==== 0.8230730655119142 accuracy==== 0.7266666666666667\n",
      "epoch 33 loss==== 0.8229928573286005 accuracy==== 0.7266666666666667\n",
      "epoch 34 loss==== 0.8229133203222789 accuracy==== 0.7266666666666667\n",
      "epoch 35 loss==== 0.8228345190662915 accuracy==== 0.7266666666666667\n",
      "epoch 36 loss==== 0.8227565101598079 accuracy==== 0.7266666666666667\n",
      "epoch 37 loss==== 0.8226793428974974 accuracy==== 0.7266666666666667\n",
      "epoch 38 loss==== 0.8226030598900487 accuracy==== 0.7266666666666667\n",
      "epoch 39 loss==== 0.8225276976384679 accuracy==== 0.7266666666666667\n",
      "epoch 40 loss==== 0.8224532870651686 accuracy==== 0.7266666666666667\n",
      "epoch 41 loss==== 0.8223798540047366 accuracy==== 0.7266666666666667\n",
      "epoch 42 loss==== 0.8223074196571716 accuracy==== 0.7266666666666667\n",
      "epoch 43 loss==== 0.8222360010062499 accuracy==== 0.7266666666666667\n",
      "epoch 44 loss==== 0.8221656112055121 accuracy==== 0.7266666666666667\n",
      "epoch 45 loss==== 0.8220962599342111 accuracy==== 0.7266666666666667\n",
      "epoch 46 loss==== 0.8220279537254169 accuracy==== 0.7266666666666667\n",
      "epoch 47 loss==== 0.821960696268292 accuracy==== 0.7266666666666667\n",
      "epoch 48 loss==== 0.8218944886864639 accuracy==== 0.7266666666666667\n",
      "epoch 49 loss==== 0.8218293297942036 accuracy==== 0.7266666666666667\n",
      "epoch 50 loss==== 0.8217652163320992 accuracy==== 0.7266666666666667\n",
      "epoch 51 loss==== 0.8217021431836796 accuracy==== 0.7266666666666667\n",
      "epoch 52 loss==== 0.8216401035744361 accuracy==== 0.7266666666666667\n",
      "epoch 53 loss==== 0.8215790892545227 accuracy==== 0.7266666666666667\n",
      "epoch 54 loss==== 0.8215190906663571 accuracy==== 0.7266666666666667\n",
      "epoch 55 loss==== 0.8214600970982243 accuracy==== 0.7266666666666667\n",
      "epoch 56 loss==== 0.8214020968249601 accuracy==== 0.7266666666666667\n",
      "epoch 57 loss==== 0.8213450772366501 accuracy==== 0.7266666666666667\n",
      "epoch 58 loss==== 0.8212890249562683 accuracy==== 0.7266666666666667\n",
      "epoch 59 loss==== 0.8212339259470809 accuracy==== 0.7266666666666667\n",
      "epoch 60 loss==== 0.8211797656105835 accuracy==== 0.7266666666666667\n",
      "epoch 61 loss==== 0.8211265288757339 accuracy==== 0.7266666666666667\n",
      "epoch 62 loss==== 0.8210742002800933 accuracy==== 0.7266666666666667\n",
      "epoch 63 loss==== 0.8210227640435522 accuracy==== 0.7266666666666667\n",
      "epoch 64 loss==== 0.8209722041352033 accuracy==== 0.7266666666666667\n",
      "epoch 65 loss==== 0.8209225043338727 accuracy==== 0.7266666666666667\n",
      "epoch 66 loss==== 0.8208736482828645 accuracy==== 0.7266666666666667\n",
      "epoch 67 loss==== 0.8208256195393343 accuracy==== 0.7266666666666667\n",
      "epoch 68 loss==== 0.8207784016187438 accuracy==== 0.7333333333333333\n",
      "epoch 69 loss==== 0.8207319780348004 accuracy==== 0.7333333333333333\n",
      "epoch 70 loss==== 0.8206863323352352 accuracy==== 0.7333333333333333\n",
      "epoch 71 loss==== 0.8206414481337799 accuracy==== 0.7333333333333333\n",
      "epoch 72 loss==== 0.820597309138653 accuracy==== 0.7333333333333333\n",
      "epoch 73 loss==== 0.8205538991778395 accuracy==== 0.7333333333333333\n",
      "epoch 74 loss==== 0.8205112022214613 accuracy==== 0.7333333333333333\n",
      "epoch 75 loss==== 0.8204692024014646 accuracy==== 0.7333333333333333\n",
      "epoch 76 loss==== 0.820427884028865 accuracy==== 0.7333333333333333\n",
      "epoch 77 loss==== 0.8203872316087785 accuracy==== 0.7333333333333333\n",
      "epoch 78 loss==== 0.8203472298534137 accuracy==== 0.7333333333333333\n",
      "epoch 79 loss==== 0.8203078636932217 accuracy==== 0.7333333333333333\n",
      "epoch 80 loss==== 0.8202691182863717 accuracy==== 0.7333333333333333\n",
      "epoch 81 loss==== 0.8202309790267053 accuracy==== 0.7333333333333333\n",
      "epoch 82 loss==== 0.8201934315503089 accuracy==== 0.7333333333333333\n",
      "epoch 83 loss==== 0.8201564617408392 accuracy==== 0.7333333333333333\n",
      "epoch 84 loss==== 0.8201200557337223 accuracy==== 0.7333333333333333\n",
      "epoch 85 loss==== 0.8200841999193379 accuracy==== 0.7333333333333333\n",
      "epoch 86 loss==== 0.8200488809452868 accuracy==== 0.7333333333333333\n",
      "epoch 87 loss==== 0.8200140857203282 accuracy==== 0.7333333333333333\n",
      "epoch 88 loss==== 0.8199798014095308 accuracy==== 0.7333333333333333\n",
      "epoch 89 loss==== 0.8199460154360031 accuracy==== 0.7333333333333333\n",
      "epoch 90 loss==== 0.8199127154837326 accuracy==== 0.7333333333333333\n",
      "epoch 91 loss==== 0.8198798894945489 accuracy==== 0.7333333333333333\n",
      "epoch 92 loss==== 0.8198475256666183 accuracy==== 0.7333333333333333\n",
      "epoch 93 loss==== 0.8198156124525146 accuracy==== 0.7333333333333333\n",
      "epoch 94 loss==== 0.8197841385569488 accuracy==== 0.7333333333333333\n",
      "epoch 95 loss==== 0.8197530929341771 accuracy==== 0.7333333333333333\n",
      "epoch 96 loss==== 0.8197224647851445 accuracy==== 0.7333333333333333\n",
      "epoch 97 loss==== 0.8196922435543941 accuracy==== 0.7333333333333333\n",
      "epoch 98 loss==== 0.8196624189267677 accuracy==== 0.7333333333333333\n",
      "epoch 99 loss==== 0.8196329808239582 accuracy==== 0.7333333333333333\n",
      "epoch 100 loss==== 0.8196039194009012 accuracy==== 0.7333333333333333\n",
      "Wall time: 8min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    model.gradient(X,y,1e-3)\n",
    "    print(\"epoch\",epoch,\\\n",
    "          \"loss====\",model.loss(X,y),\"accuracy====\",model.accuracy(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = {\n",
    "            'W1':np.random.randn(4,100),\n",
    "            'W2':np.random.randn(100,50),\n",
    "            'W3':np.random.randn(50,3)\n",
    "        }\n",
    "b = {\n",
    "            'b1':np.random.randn(100),\n",
    "            'b2':np.random.randn(50),\n",
    "            'b3':np.random.randn(3)\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.59235522e-01, -7.84704144e-01,  5.71837816e-01,\n",
       "        -1.69548853e+00, -5.85873933e-01,  1.70288059e-01,\n",
       "        -1.72925932e+00,  3.04975290e-01,  3.82538999e-01,\n",
       "        -1.00599163e+00,  9.84928628e-01, -1.86344060e+00,\n",
       "        -4.24657732e-01, -9.26831654e-01,  1.17853472e+00,\n",
       "        -2.03969123e+00, -1.79628440e+00,  3.45658892e-01,\n",
       "        -1.51111926e+00,  1.40941956e-01,  8.61506684e-01,\n",
       "        -1.14318468e+00,  1.08871237e+00, -1.36274015e+00,\n",
       "         1.15523032e+00, -9.03694334e-02,  4.51613964e-01,\n",
       "         6.53743063e-01, -2.51703458e-01, -3.73974929e-01,\n",
       "        -1.07879565e-01,  7.35744665e-01, -5.74814808e-01,\n",
       "         1.91707978e+00, -9.28388761e-01,  8.75678694e-01,\n",
       "        -8.42582859e-01,  1.28724123e+00, -1.27946223e-01,\n",
       "        -7.60566551e-01,  2.97489369e-02,  5.20705188e-01,\n",
       "         1.95425264e-01, -5.18871627e-01,  5.48623885e-01,\n",
       "         1.53796003e-01,  4.54256349e-01,  1.01408054e+00,\n",
       "        -1.91817753e+00, -6.94567788e-01,  6.56037800e-01,\n",
       "        -5.48107034e-01, -1.04173137e-01, -1.34678418e+00,\n",
       "        -6.72832579e-02,  9.74568522e-01, -1.75230210e-01,\n",
       "         1.40064169e-01, -3.74065587e-01, -5.59647244e-02,\n",
       "         4.22052891e-02, -6.79419650e-01,  5.07502685e-01,\n",
       "         8.50853273e-01, -2.14058604e+00,  9.08064270e-01,\n",
       "         1.19512419e+00, -9.55925606e-01, -1.65502957e+00,\n",
       "         1.72368668e+00, -1.22295745e+00, -4.47485347e-01,\n",
       "        -7.51493901e-01,  2.31910236e-02,  1.14863484e-01,\n",
       "        -1.53823586e-01, -5.48952246e-01,  4.82037701e-01,\n",
       "        -1.47158101e-01,  2.70025307e-01,  1.21732788e+00,\n",
       "        -7.96288509e-02,  1.10007319e+00,  8.41161409e-01,\n",
       "        -1.77393081e-02,  1.22315571e+00, -2.33075697e-01,\n",
       "         2.29401846e-01, -1.15240704e+00, -2.93490396e-01,\n",
       "        -1.27466059e+00, -1.15948988e+00,  1.01607900e+00,\n",
       "        -1.21270826e+00, -1.16302038e+00, -9.45790156e-01,\n",
       "        -8.86772782e-01, -2.36234882e-01,  1.01915205e-01,\n",
       "        -4.68400235e-01],\n",
       "       [-9.70139831e-01, -1.65584765e-01,  1.50806732e+00,\n",
       "         1.41301955e+00, -4.86538649e-01,  6.53372036e-01,\n",
       "        -2.22481432e+00, -1.41186224e+00,  5.03418484e-01,\n",
       "        -5.42010800e-02,  1.01385677e+00, -3.06585346e-01,\n",
       "        -7.32136912e-01,  1.44818693e+00,  2.05481373e-01,\n",
       "         4.58300528e-02,  3.84561648e-01,  4.81134527e-01,\n",
       "        -3.24659444e-01,  1.13802468e+00,  1.92702656e+00,\n",
       "        -1.26241816e+00,  4.27085557e-01,  7.41597864e-01,\n",
       "        -1.80382630e-01,  2.16839116e+00, -2.26296824e-01,\n",
       "        -3.62772369e-02,  4.66040962e-01,  1.28301519e+00,\n",
       "         5.63866455e-01, -1.16216331e+00, -7.40279200e-01,\n",
       "         2.98190915e-01,  3.98877616e-01, -2.11312573e+00,\n",
       "         3.03228361e-01,  1.28975383e+00,  6.84394443e-01,\n",
       "        -3.88711582e-01, -2.39991439e+00, -1.91786085e+00,\n",
       "        -3.02255662e-01, -7.00003101e-03, -1.70129763e+00,\n",
       "         1.55484437e+00, -4.30571370e-01, -1.13301536e+00,\n",
       "         5.84243677e-01, -4.43631631e-01, -2.33405817e+00,\n",
       "         1.10870860e+00, -2.20755209e-01, -8.30173803e-01,\n",
       "        -5.57285454e-01, -1.25118230e+00,  6.77534619e-01,\n",
       "        -6.23114617e-01, -1.32998708e-01,  1.36649683e-01,\n",
       "        -3.52367761e-02, -3.31468881e-01, -9.52684557e-02,\n",
       "         8.52594340e-01,  1.06342555e+00,  7.21187921e-01,\n",
       "        -1.49962770e+00, -4.16758854e-01,  5.94977870e-01,\n",
       "        -2.55666966e-03,  7.57383470e-01,  1.02783463e+00,\n",
       "        -2.14575247e-01, -1.23322484e+00, -1.89174775e+00,\n",
       "        -4.41990267e-01,  3.85975063e-01, -1.62634831e-01,\n",
       "         4.02832536e-01,  4.35126047e-01,  5.29831200e-01,\n",
       "        -9.38447776e-01, -7.41447287e-01, -1.18632156e+00,\n",
       "         1.12416792e+00,  7.96027279e-01,  8.83086710e-02,\n",
       "         1.47421475e+00, -4.48354461e-01,  4.03261271e-01,\n",
       "         4.71582412e-01,  2.34100428e-01, -8.49619429e-01,\n",
       "        -3.23013879e-01, -7.29200777e-01,  6.56411838e-01,\n",
       "        -4.51882739e-02, -9.52948293e-01,  1.27844662e+00,\n",
       "        -1.34009411e-01],\n",
       "       [ 1.90687328e-01,  9.73999609e-01, -6.00141275e-03,\n",
       "        -7.85388404e-01,  4.92932076e-01, -3.24200360e-01,\n",
       "         4.26821000e-01,  6.75023422e-01, -9.38822622e-01,\n",
       "        -1.74450060e-01,  3.74173881e-01,  1.61815696e+00,\n",
       "         1.83086104e-02,  1.73522010e+00,  3.83494465e-01,\n",
       "        -1.11331188e+00,  3.54451559e-01, -3.24301467e-01,\n",
       "         1.21260244e+00,  1.27158704e+00, -1.23697037e+00,\n",
       "         3.84279332e-01,  1.17494600e+00, -1.69410238e+00,\n",
       "         4.46948759e-01, -2.01199758e+00, -2.71006826e+00,\n",
       "         3.96912573e-01, -2.02114564e+00,  1.72669776e+00,\n",
       "         1.02406474e+00, -1.42630490e+00, -5.76268931e-01,\n",
       "         9.28253212e-02, -6.35348950e-01, -6.33953841e-01,\n",
       "         5.66886301e-01,  5.59161281e-01,  2.33722813e+00,\n",
       "        -6.96388525e-01,  4.56729145e-01, -5.91611220e-01,\n",
       "         5.29633928e-01, -3.18381841e-01, -1.30042459e+00,\n",
       "         1.43981232e+00, -3.73449032e-01,  2.84358259e-01,\n",
       "        -1.14584127e-01,  1.81882889e+00,  4.47120490e-02,\n",
       "         4.59030973e-01, -1.89371050e+00,  1.19019501e+00,\n",
       "        -2.16584747e-01,  7.59359628e-01, -3.85634652e-01,\n",
       "         6.01387658e-01,  5.87114084e-01,  4.29018552e-01,\n",
       "         6.72224903e-01, -8.25397920e-01, -8.68463724e-01,\n",
       "        -9.80132734e-01,  7.61147940e-01, -2.60275739e+00,\n",
       "        -9.80099138e-02, -1.68917351e-02,  1.70744168e-01,\n",
       "         2.02106847e-01, -1.29903807e+00, -2.42909250e-01,\n",
       "         1.97549757e-01,  4.12178232e-01,  5.45996181e-01,\n",
       "         5.66516270e-01, -1.45299068e+00, -1.52466199e+00,\n",
       "         1.00116115e+00, -2.72564784e-01, -9.77618451e-01,\n",
       "         3.25671843e-01,  1.18272559e+00, -1.38900194e+00,\n",
       "        -1.52251324e+00, -4.34553281e-01, -1.56610282e+00,\n",
       "         7.36397951e-01, -6.15099752e-01, -4.47322179e-02,\n",
       "         3.22130105e-01,  1.00298270e-01,  8.04048370e-02,\n",
       "         1.87806083e+00,  6.51515403e-01,  1.13024960e+00,\n",
       "        -8.76486266e-02,  1.17087481e+00, -5.94043538e-01,\n",
       "         2.13837430e-01],\n",
       "       [ 2.16265212e-01, -1.50652136e+00,  1.18179935e-01,\n",
       "         3.41977744e-01, -1.50286930e+00,  9.61761803e-01,\n",
       "         1.13106480e+00,  3.20926585e-01,  9.68872746e-01,\n",
       "         5.74413974e-01,  3.53568952e-01,  1.49381674e+00,\n",
       "         2.12226296e+00,  1.12707911e+00,  9.87098093e-01,\n",
       "         6.84678295e-01,  1.71918672e+00, -8.65078662e-01,\n",
       "        -5.36109689e-01, -1.57317913e+00,  2.21880092e-01,\n",
       "        -4.09968571e-01,  1.96859688e-01,  7.67146748e-01,\n",
       "        -5.47787948e-01,  3.70223978e-01,  9.19184298e-01,\n",
       "         9.28542929e-01,  7.76920188e-01, -1.24133002e+00,\n",
       "         2.16572346e-01, -2.35274802e-02, -1.16868320e+00,\n",
       "        -1.31031237e-01,  1.65592304e+00, -2.00394808e+00,\n",
       "        -2.47591719e-01, -4.40796302e-01, -1.08811775e+00,\n",
       "         8.09430877e-01, -6.02192705e-01,  6.40467909e-01,\n",
       "         2.04078938e+00, -6.26984302e-01, -4.72775358e-01,\n",
       "         1.01301704e+00, -3.85877480e-01,  8.41202941e-01,\n",
       "         2.44571602e+00, -5.43031668e-01, -1.11448380e+00,\n",
       "         5.50341730e-01, -3.63326795e-01, -4.29837098e-01,\n",
       "         4.73174949e-01,  7.58585418e-01,  1.38030579e+00,\n",
       "        -1.54530294e+00, -2.01713296e-01, -3.87502515e-01,\n",
       "         1.98245258e-01, -1.65240598e+00,  9.34061689e-01,\n",
       "         1.94986967e-01, -1.40758681e-01, -2.06257184e-01,\n",
       "         7.38966330e-01,  1.67546429e+00,  1.06767508e+00,\n",
       "        -2.87176359e-02, -2.29588332e-01, -5.26558748e-01,\n",
       "        -5.38903180e-02, -1.07308505e+00, -6.92292212e-01,\n",
       "         2.44088053e-02, -9.39744858e-01,  1.14650868e+00,\n",
       "         1.30738397e-01, -3.93152304e-01,  3.55984865e-01,\n",
       "        -4.51466844e-01, -2.07376123e-01,  7.66398372e-01,\n",
       "         1.26446025e+00,  4.02700771e-01,  6.35514757e-01,\n",
       "         1.92738052e-01,  2.44122214e-01,  2.65956181e-01,\n",
       "         6.30102352e-01,  9.69139481e-01,  8.36617963e-01,\n",
       "        -1.15630670e+00,  4.04368402e-01, -1.28653537e-01,\n",
       "         1.84088439e+00,  1.00008535e+00, -1.89855465e+00,\n",
       "         8.35098272e-01]])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.W['W1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_iris()['data']\n",
    "y = load_iris()['target']\n",
    "y = make_onehot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def __init__(self):\n",
    "        self.W = {\n",
    "            'W1':np.random.randn(4,100),\n",
    "            'W2':np.random.randn(100,50),\n",
    "            'W3':np.random.randn(50,3)\n",
    "        }\n",
    "        self.b = {\n",
    "            'b1':np.random.randn(100),\n",
    "            'b2':np.random.randn(50),\n",
    "            'b3':np.random.randn(3)\n",
    "        }\n",
    "        self.Activation ={\n",
    "            'sigmoid':Activation.sigmoid,\n",
    "            'relu':Activation.relu,\n",
    "            'softmax':Activation.softmax\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(100,activation='relu',input_shape=(4,)))\n",
    "model.add(keras.layers.Dense(50,activation='relu'))\n",
    "model.add(keras.layers.Dense(3,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 1.1674 - accuracy: 0.3333\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1320 - accuracy: 0.3333\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1130 - accuracy: 0.2333\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0987 - accuracy: 0.2067\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0862 - accuracy: 0.2467\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0746 - accuracy: 0.3000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0639 - accuracy: 0.3267\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0539 - accuracy: 0.3333\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0452 - accuracy: 0.3333\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0377 - accuracy: 0.3333\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0303 - accuracy: 0.3333\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0232 - accuracy: 0.3333\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0162 - accuracy: 0.3333\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0094 - accuracy: 0.3400\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0027 - accuracy: 0.3600\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9961 - accuracy: 0.4000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9896 - accuracy: 0.4533\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9833 - accuracy: 0.5067\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9771 - accuracy: 0.5733\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9710 - accuracy: 0.5933\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9650 - accuracy: 0.6267\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9590 - accuracy: 0.6400\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9531 - accuracy: 0.6533\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9473 - accuracy: 0.6600\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9415 - accuracy: 0.6600\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9357 - accuracy: 0.6600\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9300 - accuracy: 0.6667\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9244 - accuracy: 0.6667\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9187 - accuracy: 0.6667\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9131 - accuracy: 0.6667\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9076 - accuracy: 0.6667\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9020 - accuracy: 0.6667\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8965 - accuracy: 0.6667\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8911 - accuracy: 0.6667\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8857 - accuracy: 0.6667\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8803 - accuracy: 0.6667\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8747 - accuracy: 0.6667\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8690 - accuracy: 0.6667\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8633 - accuracy: 0.6667\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8576 - accuracy: 0.6667\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8522 - accuracy: 0.6667\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8469 - accuracy: 0.6667\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8416 - accuracy: 0.6667\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8365 - accuracy: 0.6667\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8314 - accuracy: 0.6733\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8262 - accuracy: 0.6733\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8211 - accuracy: 0.6733\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8159 - accuracy: 0.6733\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8105 - accuracy: 0.6733\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8051 - accuracy: 0.6800\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7995 - accuracy: 0.6867\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7936 - accuracy: 0.6867\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7878 - accuracy: 0.6867\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7821 - accuracy: 0.6867\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7770 - accuracy: 0.6933\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7719 - accuracy: 0.6933\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7670 - accuracy: 0.6933\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7622 - accuracy: 0.6933\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7575 - accuracy: 0.6933\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7528 - accuracy: 0.7000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7482 - accuracy: 0.7000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7436 - accuracy: 0.7000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7390 - accuracy: 0.7000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7344 - accuracy: 0.7000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7299 - accuracy: 0.7133\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7253 - accuracy: 0.7133\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7207 - accuracy: 0.7133\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7161 - accuracy: 0.7133\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7115 - accuracy: 0.7267\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7070 - accuracy: 0.7333\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7025 - accuracy: 0.7400\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6980 - accuracy: 0.7400\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6936 - accuracy: 0.7467\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6892 - accuracy: 0.7533\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6849 - accuracy: 0.7533\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6807 - accuracy: 0.7533\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6766 - accuracy: 0.7600\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6724 - accuracy: 0.7600\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6684 - accuracy: 0.7733\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6644 - accuracy: 0.7733\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6604 - accuracy: 0.7867\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6564 - accuracy: 0.7867\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6525 - accuracy: 0.7867\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6487 - accuracy: 0.8000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6448 - accuracy: 0.8000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6410 - accuracy: 0.8133\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.8133\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6336 - accuracy: 0.8200\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6299 - accuracy: 0.8200\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6262 - accuracy: 0.8200\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6225 - accuracy: 0.8267\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6189 - accuracy: 0.8267\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6154 - accuracy: 0.8267\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6118 - accuracy: 0.8333\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6083 - accuracy: 0.8333\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6049 - accuracy: 0.8333\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6015 - accuracy: 0.8333\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5981 - accuracy: 0.8400\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5947 - accuracy: 0.8467\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5913 - accuracy: 0.8467\n",
      "Wall time: 801 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bab506ac88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X,y,epochs=100,batch_size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multilayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = self.x * self.y\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = dout*self.y\n",
    "        dy = dout*self.x\n",
    "        return dx,dy\n",
    "\n",
    "class Addlayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self,x,y):\n",
    "        out = x+y\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = dout*1\n",
    "        dy = dout*1\n",
    "        return dx,dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul = Multilayer()\n",
    "add = Addlayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mul.forward(100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mul.backward(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add.forward(100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add.backward(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx  \n",
    "\n",
    "class Sigmoid:\n",
    "    def __init_(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = Activation.sigmoid(x)\n",
    "        self.out = out\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = dout*self.out*(1.-self.out)\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.original_shape = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # shape유지\n",
    "        self.original_shape = x.shape\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        self.x = x\n",
    "        out = np.dot(self.x,self.W) + self.b\n",
    "        return out\n",
    "        \n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = np.dot(dout,self.W.T)\n",
    "        self.dW = np.dot(self.x.T,dout)\n",
    "        self.db = np.sum(dout,axis=0)\n",
    "        dx = dx.reshape(*self.original_shape)\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    \n",
    "    def forward(self,x,t):\n",
    "        self.t = t\n",
    "        self.y = Activation.softmax(x)\n",
    "        self.loss = cross_entropy(self.y,self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        dx = (self.y - self.t)*dout\n",
    "        return dx\n",
    "        \n",
    "class LeakyReLu:\n",
    "    \n",
    "    def __init__(self,alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = np.where(x>0,x,self.alpha*x)\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = np.where(dout >0,dout,self.alpha*dout) \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiNet:\n",
    "    def __init__(self):\n",
    "        self.W = {\n",
    "            'W1':np.random.randn(4,100),\n",
    "            'W2':np.random.randn(100,50),\n",
    "            'W3':np.random.randn(50,3),\n",
    "            'b1':np.random.randn(100),\n",
    "            'b2':np.random.randn(50),\n",
    "            'b3':np.random.randn(3),\n",
    "        }\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.W['W1'],self.W['b1'])\n",
    "        self.layers['relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.W['W2'],self.W['b2'])\n",
    "        self.layers['relu2'] = Relu()\n",
    "        self.layers['Affine3'] = Affine(self.W['W3'],self.W['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "    \n",
    "    \n",
    "    def predict(self,x):\n",
    "        for layer in self.layers.values():\n",
    "            x  = layer.forward(x)\n",
    "        return x\n",
    "              \n",
    "    def loss(self,x,t):\n",
    "        self.y = self.predict(x)   \n",
    "        return self.last_layer.forward(y,t)\n",
    "    \n",
    "    def _numeric_gradient(self,x,t,learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        f = lambda w : self.loss(x,t)\n",
    "        for i in range(len(self.W)):\n",
    "            self.W['W'+str(i+1)] -= self.learning_rate*numerical_gradient(f,self.W['W'+str(i+1)])\n",
    "            self.b['b'+str(i+1)] -= self.learning_rate*numerical_gradient(f,self.b['b'+str(i+1)])\n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        result = self.predict(x)\n",
    "        acc = sum(np.argmax(result,axis=1) == np.argmax(t,axis=1))/len(t)\n",
    "        return acc\n",
    "    \n",
    "    def gradient(self,x,t):\n",
    "        ## forward\n",
    "        self.loss(x,t)\n",
    "        ## backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['W3'] = self.layers['Affine3'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        grads['b3'] = self.layers['Affine3'].db\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
